{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e29dfe1",
   "metadata": {},
   "source": [
    "# Extended Neighbourhoods Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11f2ed6",
   "metadata": {},
   "source": [
    "This notebook is a control to which the first attempt at implementing Extended Neighbourhoods can be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db906ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.spatial.distance import cdist\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import sparse_categorical_accuracy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from spektral.data import Dataset, Graph\n",
    "from spektral.data import BatchLoader\n",
    "from spektral.datasets.mnist import MNIST\n",
    "from spektral.layers import GCNConv, GlobalSumPool\n",
    "from spektral.utils.sparse import sp_matrix_to_sp_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e30bbd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 64  # Batch size\n",
    "epochs = 1000  # Number of training epochs\n",
    "patience = 10  # Patience for early stopping\n",
    "l2_reg = 5e-4  # Regularization rate for l2\n",
    "\n",
    "# Import MNIST dataset in pixel format\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train, x_test = x_train.astype(\"float32\")[...]/255.0, x_test.astype(\"float32\")[...]/255.0\n",
    "\n",
    "# Pixels with greater than 0.4x max brightness are considered 'on', all others are 'off'\n",
    "x_train = np.where(0.4 < x_train, 1, 0)\n",
    "x_test = np.where(0.4 < x_test, 1, 0)\n",
    "\n",
    "\n",
    "\n",
    "# Graph Generation\n",
    "# This class takes in the MNIST dataset in pixel form and converts it into a Graph format\n",
    "# The details of this format can be found here: https://graphneural.network/data/#graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67b9b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateDataset(Dataset):\n",
    "    def __init__(self, n_samples, data, labels, nbdsize =1, **kwargs):\n",
    "        self.n_samples = n_samples\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.nbdsize = nbdsize\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    \n",
    "    def read(self):\n",
    "        def make_graph(ind):\n",
    "            # Flatten the 28x28 grid into a 784 length array\n",
    "            # This will be immediately undone in the next step but is being done here anyway as practice for future projects\n",
    "            nodes = np.ndarray.flatten(self.data[ind])\n",
    "            # Isolate the indices of the bright pixels\n",
    "            bright = np.delete((np.where(nodes == 1)), -1)\n",
    "            \n",
    "            node_coords = []\n",
    "            edges = []\n",
    "            edges_2 = []\n",
    "            \n",
    "            # The divmod function returns the coordinates from the index in the 784-array\n",
    "            # he quotient yields the y-coordinate and the remainder yields the x-coordinate\n",
    "            for i in range(len(bright)):\n",
    "                node_coords.append(divmod(bright[i],28))\n",
    "            \n",
    "            # This creates a matrix of the distances between the bright pixels\n",
    "            DistMat = cdist(node_coords, node_coords)\n",
    "            \n",
    "            # This step iterates over pairs of bright pixels, and the indices of the pairs are added to 'edges' if they are within a certain distance\n",
    "            # Selecting 1.5 will result edges being created between bright pixels that are within each others' neighbourhood-of-8\n",
    "            # (Diagonal distance is sqrt(2) =~ 1.414)\n",
    "            # The distance can be increased accordingly to include larger neighbourhoods\n",
    "            for i in range(len(node_coords)):\n",
    "                if self.nbdsize == 1:\n",
    "                    for j in range(i+1, len(node_coords)):\n",
    "                        if DistMat[i][j] <= 1.5:\n",
    "                            edges.append((bright[i],bright[j]))\n",
    "                elif self.nbdsize == 2:\n",
    "                    for j in range(i+1, len(node_coords)):\n",
    "                        if DistMat[i][j] <= 1.5:\n",
    "                            edges.append((bright[i],bright[j]))\n",
    "                        elif DistMat[i][j] <= 2.3:\n",
    "                            edges_2.append((bright[i],bright[j]))\n",
    "                                \n",
    "\n",
    "            # Node features\n",
    "            x = np.array(nodes, dtype=np.float32)\n",
    "\n",
    "            # Edges\n",
    "            r, c = zip(*edges)\n",
    "            a = sp.csr_matrix((np.ones(len(r)), (np.array(r), np.array(c))), shape=(784, 784), dtype=np.float32)\n",
    "            if self.nbdsize == 2:\n",
    "                r_2, c_2 = zip(*edges_2)\n",
    "                a_2 = sp.csr_matrix(((np.ones(len(r_2))/3), (np.array(r_2), np.array(c_2))), shape=(784, 784), dtype=np.float32)\n",
    "                a = a + a_2\n",
    "            \n",
    "            # Labels\n",
    "            y = self.labels[ind]\n",
    "            \n",
    "            # Counters and Diagnostics\n",
    "            if ind == 0:\n",
    "                print(\"Graph generation started\")\n",
    "            elif (ind%100) == 0:\n",
    "                print(ind, \"graphs generated\")\n",
    "                \n",
    "            ind +=1   \n",
    "            \n",
    "            return Graph(x=x, a=a, y=y)\n",
    "                    \n",
    "        # We must return a list of Graph objects\n",
    "        return [make_graph(index) for index in range(self.n_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "015d6dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph generation started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\spektral\\data\\graph.py:67: UserWarning: x was automatically reshaped to (784, 1)\n",
      "  warnings.warn(f\"x was automatically reshaped to {x.shape}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 graphs generated\n",
      "200 graphs generated\n",
      "300 graphs generated\n",
      "400 graphs generated\n",
      "500 graphs generated\n",
      "600 graphs generated\n",
      "700 graphs generated\n",
      "800 graphs generated\n",
      "900 graphs generated\n",
      "1000 graphs generated\n",
      "1100 graphs generated\n",
      "1200 graphs generated\n",
      "1300 graphs generated\n",
      "1400 graphs generated\n",
      "1500 graphs generated\n",
      "1600 graphs generated\n",
      "1700 graphs generated\n",
      "1800 graphs generated\n",
      "1900 graphs generated\n",
      "2000 graphs generated\n",
      "2100 graphs generated\n",
      "2200 graphs generated\n",
      "2300 graphs generated\n",
      "2400 graphs generated\n",
      "2500 graphs generated\n",
      "2600 graphs generated\n",
      "2700 graphs generated\n",
      "2800 graphs generated\n",
      "2900 graphs generated\n",
      "3000 graphs generated\n",
      "3100 graphs generated\n",
      "3200 graphs generated\n",
      "3300 graphs generated\n",
      "3400 graphs generated\n",
      "3500 graphs generated\n",
      "3600 graphs generated\n",
      "3700 graphs generated\n",
      "3800 graphs generated\n",
      "3900 graphs generated\n",
      "4000 graphs generated\n",
      "4100 graphs generated\n",
      "4200 graphs generated\n",
      "4300 graphs generated\n",
      "4400 graphs generated\n",
      "4500 graphs generated\n",
      "4600 graphs generated\n",
      "4700 graphs generated\n",
      "4800 graphs generated\n",
      "4900 graphs generated\n",
      "5000 graphs generated\n",
      "5100 graphs generated\n",
      "5200 graphs generated\n",
      "5300 graphs generated\n",
      "5400 graphs generated\n",
      "5500 graphs generated\n",
      "5600 graphs generated\n",
      "5700 graphs generated\n",
      "5800 graphs generated\n",
      "5900 graphs generated\n",
      "6000 graphs generated\n",
      "6100 graphs generated\n",
      "6200 graphs generated\n",
      "6300 graphs generated\n",
      "6400 graphs generated\n",
      "6500 graphs generated\n",
      "6600 graphs generated\n",
      "6700 graphs generated\n",
      "6800 graphs generated\n",
      "6900 graphs generated\n",
      "7000 graphs generated\n",
      "7100 graphs generated\n",
      "7200 graphs generated\n",
      "7300 graphs generated\n",
      "7400 graphs generated\n",
      "7500 graphs generated\n",
      "7600 graphs generated\n",
      "7700 graphs generated\n",
      "7800 graphs generated\n",
      "7900 graphs generated\n",
      "8000 graphs generated\n",
      "8100 graphs generated\n",
      "8200 graphs generated\n",
      "8300 graphs generated\n",
      "8400 graphs generated\n",
      "8500 graphs generated\n",
      "8600 graphs generated\n",
      "8700 graphs generated\n",
      "8800 graphs generated\n",
      "8900 graphs generated\n",
      "9000 graphs generated\n",
      "9100 graphs generated\n",
      "9200 graphs generated\n",
      "9300 graphs generated\n",
      "9400 graphs generated\n",
      "9500 graphs generated\n",
      "9600 graphs generated\n",
      "9700 graphs generated\n",
      "9800 graphs generated\n",
      "9900 graphs generated\n",
      "10000 graphs generated\n",
      "10100 graphs generated\n",
      "10200 graphs generated\n",
      "10300 graphs generated\n",
      "10400 graphs generated\n",
      "10500 graphs generated\n",
      "10600 graphs generated\n",
      "10700 graphs generated\n",
      "10800 graphs generated\n",
      "10900 graphs generated\n",
      "11000 graphs generated\n",
      "11100 graphs generated\n",
      "11200 graphs generated\n",
      "11300 graphs generated\n",
      "11400 graphs generated\n",
      "11500 graphs generated\n",
      "11600 graphs generated\n",
      "11700 graphs generated\n",
      "11800 graphs generated\n",
      "11900 graphs generated\n",
      "12000 graphs generated\n",
      "12100 graphs generated\n",
      "12200 graphs generated\n",
      "12300 graphs generated\n",
      "12400 graphs generated\n",
      "12500 graphs generated\n",
      "12600 graphs generated\n",
      "12700 graphs generated\n",
      "12800 graphs generated\n",
      "12900 graphs generated\n",
      "13000 graphs generated\n",
      "13100 graphs generated\n",
      "13200 graphs generated\n",
      "13300 graphs generated\n",
      "13400 graphs generated\n",
      "13500 graphs generated\n",
      "13600 graphs generated\n",
      "13700 graphs generated\n",
      "13800 graphs generated\n",
      "13900 graphs generated\n",
      "14000 graphs generated\n",
      "14100 graphs generated\n",
      "14200 graphs generated\n",
      "14300 graphs generated\n",
      "14400 graphs generated\n",
      "14500 graphs generated\n",
      "14600 graphs generated\n",
      "14700 graphs generated\n",
      "14800 graphs generated\n",
      "14900 graphs generated\n",
      "15000 graphs generated\n",
      "15100 graphs generated\n",
      "15200 graphs generated\n",
      "15300 graphs generated\n",
      "15400 graphs generated\n",
      "15500 graphs generated\n",
      "15600 graphs generated\n",
      "15700 graphs generated\n",
      "15800 graphs generated\n",
      "15900 graphs generated\n",
      "16000 graphs generated\n",
      "16100 graphs generated\n",
      "16200 graphs generated\n",
      "16300 graphs generated\n",
      "16400 graphs generated\n",
      "16500 graphs generated\n",
      "16600 graphs generated\n",
      "16700 graphs generated\n",
      "16800 graphs generated\n",
      "16900 graphs generated\n",
      "17000 graphs generated\n",
      "17100 graphs generated\n",
      "17200 graphs generated\n",
      "17300 graphs generated\n",
      "17400 graphs generated\n",
      "17500 graphs generated\n",
      "17600 graphs generated\n",
      "17700 graphs generated\n",
      "17800 graphs generated\n",
      "17900 graphs generated\n",
      "18000 graphs generated\n",
      "18100 graphs generated\n",
      "18200 graphs generated\n",
      "18300 graphs generated\n",
      "18400 graphs generated\n",
      "18500 graphs generated\n",
      "18600 graphs generated\n",
      "18700 graphs generated\n",
      "18800 graphs generated\n",
      "18900 graphs generated\n",
      "19000 graphs generated\n",
      "19100 graphs generated\n",
      "19200 graphs generated\n",
      "19300 graphs generated\n",
      "19400 graphs generated\n",
      "19500 graphs generated\n",
      "19600 graphs generated\n",
      "19700 graphs generated\n",
      "19800 graphs generated\n",
      "19900 graphs generated\n",
      "20000 graphs generated\n",
      "20100 graphs generated\n",
      "20200 graphs generated\n",
      "20300 graphs generated\n",
      "20400 graphs generated\n",
      "20500 graphs generated\n",
      "20600 graphs generated\n",
      "20700 graphs generated\n",
      "20800 graphs generated\n",
      "20900 graphs generated\n",
      "21000 graphs generated\n",
      "21100 graphs generated\n",
      "21200 graphs generated\n",
      "21300 graphs generated\n",
      "21400 graphs generated\n",
      "21500 graphs generated\n",
      "21600 graphs generated\n",
      "21700 graphs generated\n",
      "21800 graphs generated\n",
      "21900 graphs generated\n",
      "22000 graphs generated\n",
      "22100 graphs generated\n",
      "22200 graphs generated\n",
      "22300 graphs generated\n",
      "22400 graphs generated\n",
      "22500 graphs generated\n",
      "22600 graphs generated\n",
      "22700 graphs generated\n",
      "22800 graphs generated\n",
      "22900 graphs generated\n",
      "23000 graphs generated\n",
      "23100 graphs generated\n",
      "23200 graphs generated\n",
      "23300 graphs generated\n",
      "23400 graphs generated\n",
      "23500 graphs generated\n",
      "23600 graphs generated\n",
      "23700 graphs generated\n",
      "23800 graphs generated\n",
      "23900 graphs generated\n",
      "24000 graphs generated\n",
      "24100 graphs generated\n",
      "24200 graphs generated\n",
      "24300 graphs generated\n",
      "24400 graphs generated\n",
      "24500 graphs generated\n",
      "24600 graphs generated\n",
      "24700 graphs generated\n",
      "24800 graphs generated\n",
      "24900 graphs generated\n",
      "25000 graphs generated\n",
      "25100 graphs generated\n",
      "25200 graphs generated\n",
      "25300 graphs generated\n",
      "25400 graphs generated\n",
      "25500 graphs generated\n",
      "25600 graphs generated\n",
      "25700 graphs generated\n",
      "25800 graphs generated\n",
      "25900 graphs generated\n",
      "26000 graphs generated\n",
      "26100 graphs generated\n",
      "26200 graphs generated\n",
      "26300 graphs generated\n",
      "26400 graphs generated\n",
      "26500 graphs generated\n",
      "26600 graphs generated\n",
      "26700 graphs generated\n",
      "26800 graphs generated\n",
      "26900 graphs generated\n",
      "27000 graphs generated\n",
      "27100 graphs generated\n",
      "27200 graphs generated\n",
      "27300 graphs generated\n",
      "27400 graphs generated\n",
      "27500 graphs generated\n",
      "27600 graphs generated\n",
      "27700 graphs generated\n",
      "27800 graphs generated\n",
      "27900 graphs generated\n",
      "28000 graphs generated\n",
      "28100 graphs generated\n",
      "28200 graphs generated\n",
      "28300 graphs generated\n",
      "28400 graphs generated\n",
      "28500 graphs generated\n",
      "28600 graphs generated\n",
      "28700 graphs generated\n",
      "28800 graphs generated\n",
      "28900 graphs generated\n",
      "29000 graphs generated\n",
      "29100 graphs generated\n",
      "29200 graphs generated\n",
      "29300 graphs generated\n",
      "29400 graphs generated\n",
      "29500 graphs generated\n",
      "29600 graphs generated\n",
      "29700 graphs generated\n",
      "29800 graphs generated\n",
      "29900 graphs generated\n",
      "30000 graphs generated\n",
      "30100 graphs generated\n",
      "30200 graphs generated\n",
      "30300 graphs generated\n",
      "30400 graphs generated\n",
      "30500 graphs generated\n",
      "30600 graphs generated\n",
      "30700 graphs generated\n",
      "30800 graphs generated\n",
      "30900 graphs generated\n",
      "31000 graphs generated\n",
      "31100 graphs generated\n",
      "31200 graphs generated\n",
      "31300 graphs generated\n",
      "31400 graphs generated\n",
      "31500 graphs generated\n",
      "31600 graphs generated\n",
      "31700 graphs generated\n",
      "31800 graphs generated\n",
      "31900 graphs generated\n",
      "32000 graphs generated\n",
      "32100 graphs generated\n",
      "32200 graphs generated\n",
      "32300 graphs generated\n",
      "32400 graphs generated\n",
      "32500 graphs generated\n",
      "32600 graphs generated\n",
      "32700 graphs generated\n",
      "32800 graphs generated\n",
      "32900 graphs generated\n",
      "33000 graphs generated\n",
      "33100 graphs generated\n",
      "33200 graphs generated\n",
      "33300 graphs generated\n",
      "33400 graphs generated\n",
      "33500 graphs generated\n",
      "33600 graphs generated\n",
      "33700 graphs generated\n",
      "33800 graphs generated\n",
      "33900 graphs generated\n",
      "34000 graphs generated\n",
      "34100 graphs generated\n",
      "34200 graphs generated\n",
      "34300 graphs generated\n",
      "34400 graphs generated\n",
      "34500 graphs generated\n",
      "34600 graphs generated\n",
      "34700 graphs generated\n",
      "34800 graphs generated\n",
      "34900 graphs generated\n",
      "35000 graphs generated\n",
      "35100 graphs generated\n",
      "35200 graphs generated\n",
      "35300 graphs generated\n",
      "35400 graphs generated\n",
      "35500 graphs generated\n",
      "35600 graphs generated\n",
      "35700 graphs generated\n",
      "35800 graphs generated\n",
      "35900 graphs generated\n",
      "36000 graphs generated\n",
      "36100 graphs generated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36200 graphs generated\n",
      "36300 graphs generated\n",
      "36400 graphs generated\n",
      "36500 graphs generated\n",
      "36600 graphs generated\n",
      "36700 graphs generated\n",
      "36800 graphs generated\n",
      "36900 graphs generated\n",
      "37000 graphs generated\n",
      "37100 graphs generated\n",
      "37200 graphs generated\n",
      "37300 graphs generated\n",
      "37400 graphs generated\n",
      "37500 graphs generated\n",
      "37600 graphs generated\n",
      "37700 graphs generated\n",
      "37800 graphs generated\n",
      "37900 graphs generated\n",
      "38000 graphs generated\n",
      "38100 graphs generated\n",
      "38200 graphs generated\n",
      "38300 graphs generated\n",
      "38400 graphs generated\n",
      "38500 graphs generated\n",
      "38600 graphs generated\n",
      "38700 graphs generated\n",
      "38800 graphs generated\n",
      "38900 graphs generated\n",
      "39000 graphs generated\n",
      "39100 graphs generated\n",
      "39200 graphs generated\n",
      "39300 graphs generated\n",
      "39400 graphs generated\n",
      "39500 graphs generated\n",
      "39600 graphs generated\n",
      "39700 graphs generated\n",
      "39800 graphs generated\n",
      "39900 graphs generated\n",
      "40000 graphs generated\n",
      "40100 graphs generated\n",
      "40200 graphs generated\n",
      "40300 graphs generated\n",
      "40400 graphs generated\n",
      "40500 graphs generated\n",
      "40600 graphs generated\n",
      "40700 graphs generated\n",
      "40800 graphs generated\n",
      "40900 graphs generated\n",
      "41000 graphs generated\n",
      "41100 graphs generated\n",
      "41200 graphs generated\n",
      "41300 graphs generated\n",
      "41400 graphs generated\n",
      "41500 graphs generated\n",
      "41600 graphs generated\n",
      "41700 graphs generated\n",
      "41800 graphs generated\n",
      "41900 graphs generated\n",
      "42000 graphs generated\n",
      "42100 graphs generated\n",
      "42200 graphs generated\n",
      "42300 graphs generated\n",
      "42400 graphs generated\n",
      "42500 graphs generated\n",
      "42600 graphs generated\n",
      "42700 graphs generated\n",
      "42800 graphs generated\n",
      "42900 graphs generated\n",
      "43000 graphs generated\n",
      "43100 graphs generated\n",
      "43200 graphs generated\n",
      "43300 graphs generated\n",
      "43400 graphs generated\n",
      "43500 graphs generated\n",
      "43600 graphs generated\n",
      "43700 graphs generated\n",
      "43800 graphs generated\n",
      "43900 graphs generated\n",
      "44000 graphs generated\n",
      "44100 graphs generated\n",
      "44200 graphs generated\n",
      "44300 graphs generated\n",
      "44400 graphs generated\n",
      "44500 graphs generated\n",
      "44600 graphs generated\n",
      "44700 graphs generated\n",
      "44800 graphs generated\n",
      "44900 graphs generated\n",
      "45000 graphs generated\n",
      "45100 graphs generated\n",
      "45200 graphs generated\n",
      "45300 graphs generated\n",
      "45400 graphs generated\n",
      "45500 graphs generated\n",
      "45600 graphs generated\n",
      "45700 graphs generated\n",
      "45800 graphs generated\n",
      "45900 graphs generated\n",
      "46000 graphs generated\n",
      "46100 graphs generated\n",
      "46200 graphs generated\n",
      "46300 graphs generated\n",
      "46400 graphs generated\n",
      "46500 graphs generated\n",
      "46600 graphs generated\n",
      "46700 graphs generated\n",
      "46800 graphs generated\n",
      "46900 graphs generated\n",
      "47000 graphs generated\n",
      "47100 graphs generated\n",
      "47200 graphs generated\n",
      "47300 graphs generated\n",
      "47400 graphs generated\n",
      "47500 graphs generated\n",
      "47600 graphs generated\n",
      "47700 graphs generated\n",
      "47800 graphs generated\n",
      "47900 graphs generated\n",
      "48000 graphs generated\n",
      "48100 graphs generated\n",
      "48200 graphs generated\n",
      "48300 graphs generated\n",
      "48400 graphs generated\n",
      "48500 graphs generated\n",
      "48600 graphs generated\n",
      "48700 graphs generated\n",
      "48800 graphs generated\n",
      "48900 graphs generated\n",
      "49000 graphs generated\n",
      "49100 graphs generated\n",
      "49200 graphs generated\n",
      "49300 graphs generated\n",
      "49400 graphs generated\n",
      "49500 graphs generated\n",
      "49600 graphs generated\n",
      "49700 graphs generated\n",
      "49800 graphs generated\n",
      "49900 graphs generated\n",
      "50000 graphs generated\n",
      "50100 graphs generated\n",
      "50200 graphs generated\n",
      "50300 graphs generated\n",
      "50400 graphs generated\n",
      "50500 graphs generated\n",
      "50600 graphs generated\n",
      "50700 graphs generated\n",
      "50800 graphs generated\n",
      "50900 graphs generated\n",
      "51000 graphs generated\n",
      "51100 graphs generated\n",
      "51200 graphs generated\n",
      "51300 graphs generated\n",
      "51400 graphs generated\n",
      "51500 graphs generated\n",
      "51600 graphs generated\n",
      "51700 graphs generated\n",
      "51800 graphs generated\n",
      "51900 graphs generated\n",
      "52000 graphs generated\n",
      "52100 graphs generated\n",
      "52200 graphs generated\n",
      "52300 graphs generated\n",
      "52400 graphs generated\n",
      "52500 graphs generated\n",
      "52600 graphs generated\n",
      "52700 graphs generated\n",
      "52800 graphs generated\n",
      "52900 graphs generated\n",
      "53000 graphs generated\n",
      "53100 graphs generated\n",
      "53200 graphs generated\n",
      "53300 graphs generated\n",
      "53400 graphs generated\n",
      "53500 graphs generated\n",
      "53600 graphs generated\n",
      "53700 graphs generated\n",
      "53800 graphs generated\n",
      "53900 graphs generated\n",
      "54000 graphs generated\n",
      "54100 graphs generated\n",
      "54200 graphs generated\n",
      "54300 graphs generated\n",
      "54400 graphs generated\n",
      "54500 graphs generated\n",
      "54600 graphs generated\n",
      "54700 graphs generated\n",
      "54800 graphs generated\n",
      "54900 graphs generated\n",
      "55000 graphs generated\n",
      "55100 graphs generated\n",
      "55200 graphs generated\n",
      "55300 graphs generated\n",
      "55400 graphs generated\n",
      "55500 graphs generated\n",
      "55600 graphs generated\n",
      "55700 graphs generated\n",
      "55800 graphs generated\n",
      "55900 graphs generated\n",
      "56000 graphs generated\n",
      "56100 graphs generated\n",
      "56200 graphs generated\n",
      "56300 graphs generated\n",
      "56400 graphs generated\n",
      "56500 graphs generated\n",
      "56600 graphs generated\n",
      "56700 graphs generated\n",
      "56800 graphs generated\n",
      "56900 graphs generated\n",
      "57000 graphs generated\n",
      "57100 graphs generated\n",
      "57200 graphs generated\n",
      "57300 graphs generated\n",
      "57400 graphs generated\n",
      "57500 graphs generated\n",
      "57600 graphs generated\n",
      "57700 graphs generated\n",
      "57800 graphs generated\n",
      "57900 graphs generated\n",
      "58000 graphs generated\n",
      "58100 graphs generated\n",
      "58200 graphs generated\n",
      "58300 graphs generated\n",
      "58400 graphs generated\n",
      "58500 graphs generated\n",
      "58600 graphs generated\n",
      "58700 graphs generated\n",
      "58800 graphs generated\n",
      "58900 graphs generated\n",
      "59000 graphs generated\n",
      "59100 graphs generated\n",
      "59200 graphs generated\n",
      "59300 graphs generated\n",
      "59400 graphs generated\n",
      "59500 graphs generated\n",
      "59600 graphs generated\n",
      "59700 graphs generated\n",
      "59800 graphs generated\n",
      "59900 graphs generated\n",
      "Graph generation started\n",
      "100 graphs generated\n",
      "200 graphs generated\n",
      "300 graphs generated\n",
      "400 graphs generated\n",
      "500 graphs generated\n",
      "600 graphs generated\n",
      "700 graphs generated\n",
      "800 graphs generated\n",
      "900 graphs generated\n",
      "1000 graphs generated\n",
      "1100 graphs generated\n",
      "1200 graphs generated\n",
      "1300 graphs generated\n",
      "1400 graphs generated\n",
      "1500 graphs generated\n",
      "1600 graphs generated\n",
      "1700 graphs generated\n",
      "1800 graphs generated\n",
      "1900 graphs generated\n",
      "2000 graphs generated\n",
      "2100 graphs generated\n",
      "2200 graphs generated\n",
      "2300 graphs generated\n",
      "2400 graphs generated\n",
      "2500 graphs generated\n",
      "2600 graphs generated\n",
      "2700 graphs generated\n",
      "2800 graphs generated\n",
      "2900 graphs generated\n",
      "3000 graphs generated\n",
      "3100 graphs generated\n",
      "3200 graphs generated\n",
      "3300 graphs generated\n",
      "3400 graphs generated\n",
      "3500 graphs generated\n",
      "3600 graphs generated\n",
      "3700 graphs generated\n",
      "3800 graphs generated\n",
      "3900 graphs generated\n",
      "4000 graphs generated\n",
      "4100 graphs generated\n",
      "4200 graphs generated\n",
      "4300 graphs generated\n",
      "4400 graphs generated\n",
      "4500 graphs generated\n",
      "4600 graphs generated\n",
      "4700 graphs generated\n",
      "4800 graphs generated\n",
      "4900 graphs generated\n",
      "5000 graphs generated\n",
      "5100 graphs generated\n",
      "5200 graphs generated\n",
      "5300 graphs generated\n",
      "5400 graphs generated\n",
      "5500 graphs generated\n",
      "5600 graphs generated\n",
      "5700 graphs generated\n",
      "5800 graphs generated\n",
      "5900 graphs generated\n",
      "6000 graphs generated\n",
      "6100 graphs generated\n",
      "6200 graphs generated\n",
      "6300 graphs generated\n",
      "6400 graphs generated\n",
      "6500 graphs generated\n",
      "6600 graphs generated\n",
      "6700 graphs generated\n",
      "6800 graphs generated\n",
      "6900 graphs generated\n",
      "7000 graphs generated\n",
      "7100 graphs generated\n",
      "7200 graphs generated\n",
      "7300 graphs generated\n",
      "7400 graphs generated\n",
      "7500 graphs generated\n",
      "7600 graphs generated\n",
      "7700 graphs generated\n",
      "7800 graphs generated\n",
      "7900 graphs generated\n",
      "8000 graphs generated\n",
      "8100 graphs generated\n",
      "8200 graphs generated\n",
      "8300 graphs generated\n",
      "8400 graphs generated\n",
      "8500 graphs generated\n",
      "8600 graphs generated\n",
      "8700 graphs generated\n",
      "8800 graphs generated\n",
      "8900 graphs generated\n",
      "9000 graphs generated\n",
      "9100 graphs generated\n",
      "9200 graphs generated\n",
      "9300 graphs generated\n",
      "9400 graphs generated\n",
      "9500 graphs generated\n",
      "9600 graphs generated\n",
      "9700 graphs generated\n",
      "9800 graphs generated\n",
      "9900 graphs generated\n"
     ]
    }
   ],
   "source": [
    "# Generating and shuffling the data  \n",
    "# Calling this generates the train, test, and validation datasets with randomized indices      \n",
    "traindata = GenerateDataset(len(x_train), x_train, y_train, 1)\n",
    "idxs = np.random.permutation(len(traindata))\n",
    "split_val = int(0.85 * len(traindata))\n",
    "idx_tr, idx_val = np.split(idxs, [split_val])\n",
    "data_tr = traindata[idx_tr]\n",
    "data_val = traindata[idx_val]\n",
    "\n",
    "testdata = GenerateDataset(len(x_test), x_test, y_test, 1)\n",
    "idx_test = np.random.permutation(len(testdata))\n",
    "data_test = testdata[idx_test]\n",
    "\n",
    "\n",
    "\n",
    "# Data loaders\n",
    "# See https://graphneural.network/loaders/ for further information\n",
    "loader_tr = BatchLoader(data_tr, batch_size=batch_size, epochs=epochs)\n",
    "loader_va = BatchLoader(data_val, batch_size=batch_size)\n",
    "loader_te = BatchLoader(data_test, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Build model\n",
    "class Net(Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv1 = GCNConv(16, activation=\"elu\", kernel_regularizer=l2(l2_reg))\n",
    "        self.conv2 = GCNConv(32, activation=\"elu\", kernel_regularizer=l2(l2_reg))\n",
    "        self.flatten = GlobalSumPool()\n",
    "        self.fc1 = Dense(512, activation=\"relu\")\n",
    "        self.fc2 = Dense(10, activation=\"softmax\")  # MNIST has 10 classes\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        x = self.conv2([x, a])\n",
    "        output = self.flatten(x)\n",
    "        output = self.fc1(output)\n",
    "        output = self.fc2(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7f7cd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = Net()\n",
    "optimizer = Adam()\n",
    "loss_fn = SparseCategoricalCrossentropy()\n",
    "\n",
    "# Training function\n",
    "@tf.function\n",
    "def train_on_batch(inputs, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
    "        acc = tf.reduce_mean(sparse_categorical_accuracy(target, predictions))\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(loader):\n",
    "    step = 0\n",
    "    results = []\n",
    "    for batch in loader:\n",
    "        step += 1\n",
    "        inputs, target = batch\n",
    "        predictions = model(inputs, training=False)\n",
    "        loss = loss_fn(target, predictions)\n",
    "        acc = tf.reduce_mean(sparse_categorical_accuracy(target, predictions))\n",
    "        results.append((loss, acc, len(target)))  # Keep track of batch size\n",
    "        if step == loader.steps_per_epoch:\n",
    "            results = np.array(results)\n",
    "            return np.average(results[:, :-1], 0, weights=results[:, -1])\n",
    "\n",
    "\n",
    "# Setup training\n",
    "best_val_loss = 99999\n",
    "current_patience = patience\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2bd4d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\spektral\\data\\utils.py:206: UserWarning: `x` isn't a recognized object; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  shuffle_inplace(*data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.6429, acc: 0.2491 | Valid loss: 1.7825, acc: 0.3376 | Test loss: 1.7823, acc: 0.3395\n",
      "Epoch Number: 1 Epoch Time: 3 minutes 15 seconds \n",
      "Train loss: 1.9138, acc: 0.3303 | Valid loss: 1.7287, acc: 0.3638 | Test loss: 1.7094, acc: 0.3701\n",
      "Epoch Number: 2 Epoch Time: 2 minutes 44 seconds \n",
      "Train loss: 1.7494, acc: 0.3599 | Valid loss: 1.7104, acc: 0.3767 | Test loss: 1.7048, acc: 0.3707\n",
      "Epoch Number: 3 Epoch Time: 2 minutes 44 seconds \n",
      "Train loss: 1.6922, acc: 0.3746 | Valid loss: 1.6434, acc: 0.3728 | Test loss: 1.6252, acc: 0.3755\n",
      "Epoch Number: 4 Epoch Time: 2 minutes 46 seconds \n",
      "Train loss: 1.6682, acc: 0.3807 | Valid loss: 1.5991, acc: 0.3981 | Test loss: 1.5892, acc: 0.4025\n",
      "Epoch Number: 5 Epoch Time: 2 minutes 47 seconds \n",
      "Train loss: 1.6575, acc: 0.3828 | Valid loss: 1.6574, acc: 0.3811 | Test loss: 1.5892, acc: 0.4025\n",
      "Epoch Number: 6 Epoch Time: 2 minutes 23 seconds \n",
      "Train loss: 1.6469, acc: 0.3846 | Valid loss: 1.6528, acc: 0.3876 | Test loss: 1.5892, acc: 0.4025\n",
      "Epoch Number: 7 Epoch Time: 2 minutes 23 seconds \n",
      "Train loss: 1.6313, acc: 0.3910 | Valid loss: 1.6098, acc: 0.3941 | Test loss: 1.5892, acc: 0.4025\n",
      "Epoch Number: 8 Epoch Time: 2 minutes 22 seconds \n",
      "Train loss: 1.6102, acc: 0.3984 | Valid loss: 1.5864, acc: 0.4054 | Test loss: 1.5773, acc: 0.4044\n",
      "Epoch Number: 9 Epoch Time: 2 minutes 46 seconds \n",
      "Train loss: 1.6037, acc: 0.4003 | Valid loss: 1.5725, acc: 0.4099 | Test loss: 1.5538, acc: 0.4099\n",
      "Epoch Number: 10 Epoch Time: 2 minutes 49 seconds \n",
      "Train loss: 1.5982, acc: 0.4021 | Valid loss: 1.6509, acc: 0.3731 | Test loss: 1.5538, acc: 0.4099\n",
      "Epoch Number: 11 Epoch Time: 2 minutes 17 seconds \n",
      "Train loss: 1.5995, acc: 0.4029 | Valid loss: 1.6655, acc: 0.3860 | Test loss: 1.5538, acc: 0.4099\n",
      "Epoch Number: 12 Epoch Time: 2 minutes 18 seconds \n",
      "Train loss: 1.5825, acc: 0.4107 | Valid loss: 1.6384, acc: 0.3994 | Test loss: 1.5538, acc: 0.4099\n",
      "Epoch Number: 13 Epoch Time: 2 minutes 19 seconds \n",
      "Train loss: 1.5879, acc: 0.4083 | Valid loss: 1.5599, acc: 0.4100 | Test loss: 1.5461, acc: 0.4076\n",
      "Epoch Number: 14 Epoch Time: 2 minutes 44 seconds \n",
      "Train loss: 1.5744, acc: 0.4135 | Valid loss: 1.5590, acc: 0.4089 | Test loss: 1.5491, acc: 0.4095\n",
      "Epoch Number: 15 Epoch Time: 2 minutes 48 seconds \n",
      "Train loss: 1.5668, acc: 0.4156 | Valid loss: 1.5223, acc: 0.4259 | Test loss: 1.5128, acc: 0.4324\n",
      "Epoch Number: 16 Epoch Time: 2 minutes 58 seconds \n",
      "Train loss: 1.5576, acc: 0.4191 | Valid loss: 1.5446, acc: 0.4154 | Test loss: 1.5128, acc: 0.4324\n",
      "Epoch Number: 17 Epoch Time: 2 minutes 23 seconds \n",
      "Train loss: 1.5546, acc: 0.4210 | Valid loss: 1.5466, acc: 0.4167 | Test loss: 1.5128, acc: 0.4324\n",
      "Epoch Number: 18 Epoch Time: 2 minutes 21 seconds \n",
      "Train loss: 1.5489, acc: 0.4226 | Valid loss: 1.5143, acc: 0.4316 | Test loss: 1.4960, acc: 0.4416\n",
      "Epoch Number: 19 Epoch Time: 2 minutes 39 seconds \n",
      "Train loss: 1.5351, acc: 0.4284 | Valid loss: 1.5143, acc: 0.4343 | Test loss: 1.4960, acc: 0.4416\n",
      "Epoch Number: 20 Epoch Time: 2 minutes 1 seconds \n",
      "Train loss: 1.5295, acc: 0.4288 | Valid loss: 1.4838, acc: 0.4414 | Test loss: 1.4620, acc: 0.4423\n",
      "Epoch Number: 21 Epoch Time: 2 minutes 13 seconds \n",
      "Train loss: 1.5219, acc: 0.4341 | Valid loss: 1.5089, acc: 0.4287 | Test loss: 1.4620, acc: 0.4423\n",
      "Epoch Number: 22 Epoch Time: 1 minutes 54 seconds \n",
      "Train loss: 1.5056, acc: 0.4409 | Valid loss: 1.5173, acc: 0.4191 | Test loss: 1.4620, acc: 0.4423\n",
      "Epoch Number: 23 Epoch Time: 2 minutes 8 seconds \n",
      "Train loss: 1.4956, acc: 0.4431 | Valid loss: 1.4611, acc: 0.4576 | Test loss: 1.4615, acc: 0.4520\n",
      "Epoch Number: 24 Epoch Time: 2 minutes 34 seconds \n",
      "Train loss: 1.4917, acc: 0.4423 | Valid loss: 1.4695, acc: 0.4523 | Test loss: 1.4615, acc: 0.4520\n",
      "Epoch Number: 25 Epoch Time: 2 minutes 10 seconds \n",
      "Train loss: 1.4867, acc: 0.4450 | Valid loss: 1.4543, acc: 0.4510 | Test loss: 1.4471, acc: 0.4577\n",
      "Epoch Number: 26 Epoch Time: 2 minutes 29 seconds \n",
      "Train loss: 1.4785, acc: 0.4481 | Valid loss: 1.4604, acc: 0.4557 | Test loss: 1.4471, acc: 0.4577\n",
      "Epoch Number: 27 Epoch Time: 2 minutes 6 seconds \n",
      "Train loss: 1.4766, acc: 0.4441 | Valid loss: 1.4532, acc: 0.4552 | Test loss: 1.4433, acc: 0.4544\n",
      "Epoch Number: 28 Epoch Time: 2 minutes 29 seconds \n",
      "Train loss: 1.4778, acc: 0.4463 | Valid loss: 1.4383, acc: 0.4562 | Test loss: 1.4321, acc: 0.4566\n",
      "Epoch Number: 29 Epoch Time: 2 minutes 32 seconds \n",
      "Train loss: 1.4685, acc: 0.4504 | Valid loss: 1.4380, acc: 0.4539 | Test loss: 1.4306, acc: 0.4622\n",
      "Epoch Number: 30 Epoch Time: 2 minutes 44 seconds \n",
      "Train loss: 1.4640, acc: 0.4528 | Valid loss: 1.4744, acc: 0.4386 | Test loss: 1.4306, acc: 0.4622\n",
      "Epoch Number: 31 Epoch Time: 2 minutes 19 seconds \n",
      "Train loss: 1.4667, acc: 0.4525 | Valid loss: 1.4934, acc: 0.4414 | Test loss: 1.4306, acc: 0.4622\n",
      "Epoch Number: 32 Epoch Time: 2 minutes 20 seconds \n",
      "Train loss: 1.4564, acc: 0.4567 | Valid loss: 1.4158, acc: 0.4666 | Test loss: 1.4115, acc: 0.4672\n",
      "Epoch Number: 33 Epoch Time: 2 minutes 43 seconds \n",
      "Train loss: 1.4575, acc: 0.4532 | Valid loss: 1.4510, acc: 0.4551 | Test loss: 1.4115, acc: 0.4672\n",
      "Epoch Number: 34 Epoch Time: 2 minutes 22 seconds \n",
      "Train loss: 1.4460, acc: 0.4618 | Valid loss: 1.4647, acc: 0.4452 | Test loss: 1.4115, acc: 0.4672\n",
      "Epoch Number: 35 Epoch Time: 2 minutes 25 seconds \n",
      "Train loss: 1.4457, acc: 0.4612 | Valid loss: 1.4353, acc: 0.4606 | Test loss: 1.4115, acc: 0.4672\n",
      "Epoch Number: 36 Epoch Time: 2 minutes 22 seconds \n",
      "Train loss: 1.4449, acc: 0.4596 | Valid loss: 1.5149, acc: 0.4310 | Test loss: 1.4115, acc: 0.4672\n",
      "Epoch Number: 37 Epoch Time: 2 minutes 21 seconds \n",
      "Train loss: 1.4437, acc: 0.4605 | Valid loss: 1.4015, acc: 0.4729 | Test loss: 1.3954, acc: 0.4766\n",
      "Epoch Number: 38 Epoch Time: 2 minutes 51 seconds \n",
      "Train loss: 1.4418, acc: 0.4615 | Valid loss: 1.4138, acc: 0.4652 | Test loss: 1.3954, acc: 0.4766\n",
      "Epoch Number: 39 Epoch Time: 2 minutes 21 seconds \n",
      "Train loss: 1.4421, acc: 0.4599 | Valid loss: 1.4754, acc: 0.4349 | Test loss: 1.3954, acc: 0.4766\n",
      "Epoch Number: 40 Epoch Time: 2 minutes 25 seconds \n",
      "Train loss: 1.4343, acc: 0.4658 | Valid loss: 1.5180, acc: 0.4257 | Test loss: 1.3954, acc: 0.4766\n",
      "Epoch Number: 41 Epoch Time: 2 minutes 22 seconds \n",
      "Train loss: 1.4364, acc: 0.4615 | Valid loss: 1.4447, acc: 0.4517 | Test loss: 1.3954, acc: 0.4766\n",
      "Epoch Number: 42 Epoch Time: 2 minutes 20 seconds \n",
      "Train loss: 1.4337, acc: 0.4637 | Valid loss: 1.4052, acc: 0.4722 | Test loss: 1.3954, acc: 0.4766\n",
      "Epoch Number: 43 Epoch Time: 2 minutes 23 seconds \n",
      "Train loss: 1.4368, acc: 0.4653 | Valid loss: 1.4366, acc: 0.4557 | Test loss: 1.3954, acc: 0.4766\n",
      "Epoch Number: 44 Epoch Time: 2 minutes 30 seconds \n",
      "Train loss: 1.4256, acc: 0.4676 | Valid loss: 1.4108, acc: 0.4658 | Test loss: 1.3954, acc: 0.4766\n",
      "Epoch Number: 45 Epoch Time: 2 minutes 49 seconds \n",
      "Train loss: 1.4216, acc: 0.4684 | Valid loss: 1.3954, acc: 0.4784 | Test loss: 1.3856, acc: 0.4738\n",
      "Epoch Number: 46 Epoch Time: 2 minutes 49 seconds \n",
      "Train loss: 1.4141, acc: 0.4718 | Valid loss: 1.4068, acc: 0.4623 | Test loss: 1.3856, acc: 0.4738\n",
      "Epoch Number: 47 Epoch Time: 2 minutes 13 seconds \n",
      "Train loss: 1.4210, acc: 0.4687 | Valid loss: 1.3948, acc: 0.4717 | Test loss: 1.3941, acc: 0.4732\n",
      "Epoch Number: 48 Epoch Time: 2 minutes 44 seconds \n",
      "Train loss: 1.4117, acc: 0.4727 | Valid loss: 1.4771, acc: 0.4426 | Test loss: 1.3941, acc: 0.4732\n",
      "Epoch Number: 49 Epoch Time: 2 minutes 50 seconds \n",
      "Train loss: 1.4223, acc: 0.4685 | Valid loss: 1.4024, acc: 0.4662 | Test loss: 1.3941, acc: 0.4732\n",
      "Epoch Number: 50 Epoch Time: 2 minutes 49 seconds \n",
      "Train loss: 1.4126, acc: 0.4719 | Valid loss: 1.3755, acc: 0.4838 | Test loss: 1.3701, acc: 0.4844\n",
      "Epoch Number: 51 Epoch Time: 3 minutes 17 seconds \n",
      "Train loss: 1.4095, acc: 0.4742 | Valid loss: 1.4139, acc: 0.4646 | Test loss: 1.3701, acc: 0.4844\n",
      "Epoch Number: 52 Epoch Time: 2 minutes 47 seconds \n",
      "Train loss: 1.4043, acc: 0.4764 | Valid loss: 1.4092, acc: 0.4651 | Test loss: 1.3701, acc: 0.4844\n",
      "Epoch Number: 53 Epoch Time: 2 minutes 47 seconds \n",
      "Train loss: 1.4003, acc: 0.4751 | Valid loss: 1.3652, acc: 0.4862 | Test loss: 1.3510, acc: 0.4891\n",
      "Epoch Number: 54 Epoch Time: 3 minutes 16 seconds \n",
      "Train loss: 1.4056, acc: 0.4736 | Valid loss: 1.3892, acc: 0.4634 | Test loss: 1.3510, acc: 0.4891\n",
      "Epoch Number: 55 Epoch Time: 2 minutes 47 seconds \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.3906, acc: 0.4818 | Valid loss: 1.3775, acc: 0.4882 | Test loss: 1.3510, acc: 0.4891\n",
      "Epoch Number: 56 Epoch Time: 2 minutes 56 seconds \n",
      "Train loss: 1.3901, acc: 0.4808 | Valid loss: 1.3814, acc: 0.4787 | Test loss: 1.3510, acc: 0.4891\n",
      "Epoch Number: 57 Epoch Time: 2 minutes 48 seconds \n",
      "Train loss: 1.3951, acc: 0.4773 | Valid loss: 1.3643, acc: 0.4892 | Test loss: 1.3642, acc: 0.4934\n",
      "Epoch Number: 58 Epoch Time: 3 minutes 15 seconds \n",
      "Train loss: 1.3923, acc: 0.4806 | Valid loss: 1.3559, acc: 0.4870 | Test loss: 1.3534, acc: 0.4805\n",
      "Epoch Number: 59 Epoch Time: 3 minutes 15 seconds \n",
      "Train loss: 1.3904, acc: 0.4833 | Valid loss: 1.3508, acc: 0.4936 | Test loss: 1.3470, acc: 0.4972\n",
      "Epoch Number: 60 Epoch Time: 3 minutes 18 seconds \n",
      "Train loss: 1.3828, acc: 0.4843 | Valid loss: 1.3470, acc: 0.4983 | Test loss: 1.3440, acc: 0.4971\n",
      "Epoch Number: 61 Epoch Time: 3 minutes 13 seconds \n",
      "Train loss: 1.3772, acc: 0.4861 | Valid loss: 1.3431, acc: 0.4902 | Test loss: 1.3287, acc: 0.4890\n",
      "Epoch Number: 62 Epoch Time: 3 minutes 15 seconds \n",
      "Train loss: 1.3745, acc: 0.4867 | Valid loss: 1.3569, acc: 0.4918 | Test loss: 1.3287, acc: 0.4890\n",
      "Epoch Number: 63 Epoch Time: 2 minutes 48 seconds \n",
      "Train loss: 1.3722, acc: 0.4869 | Valid loss: 1.3266, acc: 0.4984 | Test loss: 1.3172, acc: 0.4987\n",
      "Epoch Number: 64 Epoch Time: 3 minutes 19 seconds \n",
      "Train loss: 1.3637, acc: 0.4919 | Valid loss: 1.3498, acc: 0.4911 | Test loss: 1.3172, acc: 0.4987\n",
      "Epoch Number: 65 Epoch Time: 2 minutes 48 seconds \n",
      "Train loss: 1.3650, acc: 0.4892 | Valid loss: 1.3399, acc: 0.4973 | Test loss: 1.3172, acc: 0.4987\n",
      "Epoch Number: 66 Epoch Time: 2 minutes 51 seconds \n",
      "Train loss: 1.3594, acc: 0.4941 | Valid loss: 1.3075, acc: 0.5077 | Test loss: 1.3073, acc: 0.5100\n",
      "Epoch Number: 67 Epoch Time: 3 minutes 18 seconds \n",
      "Train loss: 1.3596, acc: 0.4926 | Valid loss: 1.4315, acc: 0.4569 | Test loss: 1.3073, acc: 0.5100\n",
      "Epoch Number: 68 Epoch Time: 2 minutes 49 seconds \n",
      "Train loss: 1.3572, acc: 0.4960 | Valid loss: 1.3459, acc: 0.4949 | Test loss: 1.3073, acc: 0.5100\n",
      "Epoch Number: 69 Epoch Time: 2 minutes 45 seconds \n",
      "Train loss: 1.3554, acc: 0.4958 | Valid loss: 1.3370, acc: 0.5042 | Test loss: 1.3073, acc: 0.5100\n",
      "Epoch Number: 70 Epoch Time: 2 minutes 49 seconds \n",
      "Train loss: 1.3449, acc: 0.5006 | Valid loss: 1.4142, acc: 0.4571 | Test loss: 1.3073, acc: 0.5100\n",
      "Epoch Number: 71 Epoch Time: 2 minutes 49 seconds \n",
      "Train loss: 1.3421, acc: 0.5004 | Valid loss: 1.3650, acc: 0.4923 | Test loss: 1.3073, acc: 0.5100\n",
      "Epoch Number: 72 Epoch Time: 2 minutes 50 seconds \n",
      "Train loss: 1.3426, acc: 0.5015 | Valid loss: 1.3151, acc: 0.5138 | Test loss: 1.3073, acc: 0.5100\n",
      "Epoch Number: 73 Epoch Time: 2 minutes 47 seconds \n",
      "Train loss: 1.3398, acc: 0.5036 | Valid loss: 1.2863, acc: 0.5286 | Test loss: 1.2844, acc: 0.5278\n",
      "Epoch Number: 74 Epoch Time: 3 minutes 16 seconds \n",
      "Train loss: 1.3313, acc: 0.5053 | Valid loss: 1.2784, acc: 0.5250 | Test loss: 1.2692, acc: 0.5303\n",
      "Epoch Number: 75 Epoch Time: 3 minutes 11 seconds \n",
      "Train loss: 1.3437, acc: 0.4985 | Valid loss: 1.3078, acc: 0.5059 | Test loss: 1.2692, acc: 0.5303\n",
      "Epoch Number: 76 Epoch Time: 2 minutes 46 seconds \n",
      "Train loss: 1.3297, acc: 0.5060 | Valid loss: 1.3363, acc: 0.4866 | Test loss: 1.2692, acc: 0.5303\n",
      "Epoch Number: 77 Epoch Time: 2 minutes 47 seconds \n",
      "Train loss: 1.3241, acc: 0.5085 | Valid loss: 1.3863, acc: 0.4813 | Test loss: 1.2692, acc: 0.5303\n",
      "Epoch Number: 78 Epoch Time: 2 minutes 47 seconds \n",
      "Train loss: 1.3386, acc: 0.5039 | Valid loss: 1.2881, acc: 0.5138 | Test loss: 1.2692, acc: 0.5303\n",
      "Epoch Number: 79 Epoch Time: 2 minutes 45 seconds \n",
      "Train loss: 1.3273, acc: 0.5069 | Valid loss: 1.2887, acc: 0.5156 | Test loss: 1.2692, acc: 0.5303\n",
      "Epoch Number: 80 Epoch Time: 2 minutes 39 seconds \n",
      "Train loss: 1.3240, acc: 0.5073 | Valid loss: 1.3605, acc: 0.4758 | Test loss: 1.2692, acc: 0.5303\n",
      "Epoch Number: 81 Epoch Time: 2 minutes 8 seconds \n",
      "Train loss: 1.3174, acc: 0.5100 | Valid loss: 1.3179, acc: 0.5060 | Test loss: 1.2692, acc: 0.5303\n",
      "Epoch Number: 82 Epoch Time: 2 minutes 8 seconds \n",
      "Train loss: 1.3176, acc: 0.5097 | Valid loss: 1.2809, acc: 0.5184 | Test loss: 1.2692, acc: 0.5303\n",
      "Epoch Number: 83 Epoch Time: 2 minutes 7 seconds \n",
      "Train loss: 1.3107, acc: 0.5144 | Valid loss: 1.3063, acc: 0.5079 | Test loss: 1.2692, acc: 0.5303\n",
      "Epoch Number: 84 Epoch Time: 2 minutes 7 seconds \n",
      "Train loss: 1.3202, acc: 0.5100 | Valid loss: 1.2676, acc: 0.5261 | Test loss: 1.2673, acc: 0.5252\n",
      "Epoch Number: 85 Epoch Time: 2 minutes 29 seconds \n",
      "Train loss: 1.3087, acc: 0.5162 | Valid loss: 1.3252, acc: 0.5076 | Test loss: 1.2673, acc: 0.5252\n",
      "Epoch Number: 86 Epoch Time: 2 minutes 7 seconds \n",
      "Train loss: 1.3093, acc: 0.5137 | Valid loss: 1.3680, acc: 0.4846 | Test loss: 1.2673, acc: 0.5252\n",
      "Epoch Number: 87 Epoch Time: 2 minutes 7 seconds \n",
      "Train loss: 1.3046, acc: 0.5175 | Valid loss: 1.2619, acc: 0.5226 | Test loss: 1.2588, acc: 0.5303\n",
      "Epoch Number: 88 Epoch Time: 2 minutes 30 seconds \n",
      "Train loss: 1.3109, acc: 0.5142 | Valid loss: 1.2757, acc: 0.5290 | Test loss: 1.2588, acc: 0.5303\n",
      "Epoch Number: 89 Epoch Time: 2 minutes 7 seconds \n",
      "Train loss: 1.3116, acc: 0.5145 | Valid loss: 1.3199, acc: 0.5140 | Test loss: 1.2588, acc: 0.5303\n",
      "Epoch Number: 90 Epoch Time: 2 minutes 7 seconds \n",
      "Train loss: 1.3056, acc: 0.5179 | Valid loss: 1.2456, acc: 0.5421 | Test loss: 1.2423, acc: 0.5401\n",
      "Epoch Number: 91 Epoch Time: 2 minutes 30 seconds \n",
      "Train loss: 1.2974, acc: 0.5182 | Valid loss: 1.2772, acc: 0.5238 | Test loss: 1.2423, acc: 0.5401\n",
      "Epoch Number: 92 Epoch Time: 2 minutes 7 seconds \n",
      "Train loss: 1.3070, acc: 0.5170 | Valid loss: 1.3234, acc: 0.5077 | Test loss: 1.2423, acc: 0.5401\n",
      "Epoch Number: 93 Epoch Time: 2 minutes 9 seconds \n",
      "Train loss: 1.2936, acc: 0.5191 | Valid loss: 1.2819, acc: 0.5144 | Test loss: 1.2423, acc: 0.5401\n",
      "Epoch Number: 94 Epoch Time: 2 minutes 8 seconds \n",
      "Train loss: 1.2994, acc: 0.5197 | Valid loss: 1.2653, acc: 0.5300 | Test loss: 1.2423, acc: 0.5401\n",
      "Epoch Number: 95 Epoch Time: 2 minutes 7 seconds \n",
      "Train loss: 1.2937, acc: 0.5226 | Valid loss: 1.2454, acc: 0.5374 | Test loss: 1.2364, acc: 0.5392\n",
      "Epoch Number: 96 Epoch Time: 2 minutes 29 seconds \n",
      "Train loss: 1.2913, acc: 0.5233 | Valid loss: 1.2523, acc: 0.5299 | Test loss: 1.2364, acc: 0.5392\n",
      "Epoch Number: 97 Epoch Time: 2 minutes 7 seconds \n",
      "Train loss: 1.2979, acc: 0.5193 | Valid loss: 1.2905, acc: 0.5143 | Test loss: 1.2364, acc: 0.5392\n",
      "Epoch Number: 98 Epoch Time: 2 minutes 7 seconds \n",
      "Train loss: 1.2935, acc: 0.5222 | Valid loss: 1.2321, acc: 0.5416 | Test loss: 1.2348, acc: 0.5432\n",
      "Epoch Number: 99 Epoch Time: 2 minutes 29 seconds \n",
      "Train loss: 1.3022, acc: 0.5179 | Valid loss: 1.2686, acc: 0.5271 | Test loss: 1.2348, acc: 0.5432\n",
      "Epoch Number: 100 Epoch Time: 2 minutes 8 seconds \n",
      "Train loss: 1.2926, acc: 0.5205 | Valid loss: 1.2635, acc: 0.5251 | Test loss: 1.2348, acc: 0.5432\n",
      "Epoch Number: 101 Epoch Time: 2 minutes 9 seconds \n",
      "Train loss: 1.2919, acc: 0.5225 | Valid loss: 1.2446, acc: 0.5328 | Test loss: 1.2348, acc: 0.5432\n",
      "Epoch Number: 102 Epoch Time: 2 minutes 9 seconds \n",
      "Train loss: 1.2826, acc: 0.5258 | Valid loss: 1.2463, acc: 0.5399 | Test loss: 1.2348, acc: 0.5432\n",
      "Epoch Number: 103 Epoch Time: 2 minutes 9 seconds \n",
      "Train loss: 1.2834, acc: 0.5254 | Valid loss: 1.2781, acc: 0.5228 | Test loss: 1.2348, acc: 0.5432\n",
      "Epoch Number: 104 Epoch Time: 2 minutes 9 seconds \n",
      "Train loss: 1.2838, acc: 0.5255 | Valid loss: 1.2547, acc: 0.5317 | Test loss: 1.2348, acc: 0.5432\n",
      "Epoch Number: 105 Epoch Time: 2 minutes 8 seconds \n",
      "Train loss: 1.2845, acc: 0.5228 | Valid loss: 1.2884, acc: 0.5250 | Test loss: 1.2348, acc: 0.5432\n",
      "Epoch Number: 106 Epoch Time: 2 minutes 8 seconds \n",
      "Train loss: 1.2891, acc: 0.5243 | Valid loss: 1.2281, acc: 0.5392 | Test loss: 1.2249, acc: 0.5424\n",
      "Epoch Number: 107 Epoch Time: 2 minutes 30 seconds \n",
      "Train loss: 1.2779, acc: 0.5254 | Valid loss: 1.2272, acc: 0.5441 | Test loss: 1.2195, acc: 0.5521\n",
      "Epoch Number: 108 Epoch Time: 2 minutes 31 seconds \n",
      "Train loss: 1.2761, acc: 0.5277 | Valid loss: 1.2573, acc: 0.5311 | Test loss: 1.2195, acc: 0.5521\n",
      "Epoch Number: 109 Epoch Time: 2 minutes 9 seconds \n",
      "Train loss: 1.2829, acc: 0.5255 | Valid loss: 1.3001, acc: 0.5133 | Test loss: 1.2195, acc: 0.5521\n",
      "Epoch Number: 110 Epoch Time: 2 minutes 8 seconds \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.2736, acc: 0.5284 | Valid loss: 1.2962, acc: 0.5181 | Test loss: 1.2195, acc: 0.5521\n",
      "Epoch Number: 111 Epoch Time: 2 minutes 9 seconds \n",
      "Train loss: 1.2698, acc: 0.5320 | Valid loss: 1.2822, acc: 0.5098 | Test loss: 1.2195, acc: 0.5521\n",
      "Epoch Number: 112 Epoch Time: 2 minutes 8 seconds \n",
      "Train loss: 1.2721, acc: 0.5283 | Valid loss: 1.2584, acc: 0.5281 | Test loss: 1.2195, acc: 0.5521\n",
      "Epoch Number: 113 Epoch Time: 2 minutes 8 seconds \n",
      "Train loss: 1.2757, acc: 0.5274 | Valid loss: 1.2393, acc: 0.5407 | Test loss: 1.2195, acc: 0.5521\n",
      "Epoch Number: 114 Epoch Time: 2 minutes 9 seconds \n",
      "Train loss: 1.2775, acc: 0.5292 | Valid loss: 1.3119, acc: 0.5064 | Test loss: 1.2195, acc: 0.5521\n",
      "Epoch Number: 115 Epoch Time: 2 minutes 9 seconds \n",
      "Train loss: 1.2627, acc: 0.5362 | Valid loss: 1.2429, acc: 0.5249 | Test loss: 1.2195, acc: 0.5521\n",
      "Epoch Number: 116 Epoch Time: 2 minutes 9 seconds \n",
      "Train loss: 1.2723, acc: 0.5301 | Valid loss: 1.2571, acc: 0.5216 | Test loss: 1.2195, acc: 0.5521\n",
      "Epoch Number: 117 Epoch Time: 2 minutes 9 seconds \n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "results_tr = []\n",
    "start_time = time.process_time()\n",
    "e_num = 1\n",
    "for batch in loader_tr:\n",
    "    step += 1\n",
    "\n",
    "    # Training step\n",
    "    inputs, target = batch\n",
    "    loss, acc = train_on_batch(inputs, target)\n",
    "    results_tr.append((loss, acc, len(target)))\n",
    "\n",
    "    if step == loader_tr.steps_per_epoch:\n",
    "        results_va = evaluate(loader_va)\n",
    "        if results_va[0] < best_val_loss:\n",
    "            best_val_loss = results_va[0]\n",
    "            current_patience = patience\n",
    "            results_te = evaluate(loader_te)\n",
    "        else:\n",
    "            current_patience -= 1\n",
    "            if current_patience == 0:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        # Print results\n",
    "        results_tr = np.array(results_tr)\n",
    "        results_tr = np.average(results_tr[:, :-1], 0, weights=results_tr[:, -1])\n",
    "        end_time = time.process_time()\n",
    "        process_time = end_time - start_time\n",
    "        print(\n",
    "            \"Train loss: {:.4f}, acc: {:.4f} | \"\n",
    "            \"Valid loss: {:.4f}, acc: {:.4f} | \"\n",
    "            \"Test loss: {:.4f}, acc: {:.4f}\".format(\n",
    "                *results_tr, *results_va, *results_te\n",
    "            )\n",
    "        )\n",
    "        print(\n",
    "            \"Epoch Number: {:.0f}, Epoch Time: {:.0f} minutes {:.0f} seconds \".format(\n",
    "                e_num, (process_time // 60), (process_time % 60)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Reset epoch\n",
    "        results_tr = []\n",
    "        step = 0\n",
    "        e_num += 1\n",
    "        start_time = time.process_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ddf68a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
